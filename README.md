# ğŸ’¼ FinSight RAG - AI-Powered Fundermental Analysis for S&P MidCap 400
[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115.0-green.svg)](https://fastapi.tiangolo.com/)
[![LangChain](https://img.shields.io/badge/LangChain-0.3.7-orange.svg)](https://www.langchain.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Hugging Face](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces)

A Retrieval-Augmented Generation (RAG) application that answers questions about financial statements using AI. Built with LangChain, FastAPI, Zilliz vector database, and OpenAI GPT-4o, featuring advanced caching, conversation memory, and production-ready architecture.

![FinSight Demo](https://via.placeholder.com/800x400/667eea/ffffff?text=FinSight+RAG+Demo)

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Architecture](#-architecture)
- [Technology Stack](#-technology-stack)
- [What I Built](#-what-i-built)
  - [Core RAG Pipeline](#1-core-rag-pipeline)
  - [Conversation Memory System](#2-conversation-memory-system)
  - [Multi-Layer Caching](#3-multi-layer-caching-system)
  - [Production Features](#4-production-features)
- [Project Structure](#-project-structure)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [Usage](#-usage)
- [API Documentation](#-api-documentation)
- [Deployment](#-deployment)
- [Performance & Cost Analysis](#-performance--cost-analysis)
- [Advanced Features](#-advanced-features)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)
- [License](#-license)
- [Acknowledgments](#-acknowledgments)

## ğŸŒŸ Features

### Core Capabilities
- ğŸ’¬ **Conversational AI** - Multi-turn conversations with context memory
- ğŸ” **Hybrid Search** - Semantic similarity + metadata filtering for precise retrieval
- âš¡ **Smart Caching** - Reduces API costs by 40-70% with intelligent cache invalidation
- ğŸ“Š **Financial Analysis** - Calculates ratios, trends, and year-over-year comparisons
- ğŸ“ **Source Citations** - Every answer includes verifiable source references
- ğŸ¯ **Query Expansion** - Generates alternative phrasings for comprehensive results
- ğŸ”„ **MMR Reranking** - Balances relevance and diversity using Maximal Marginal Relevance
- ğŸ“¦ **Contextual Compression** - Extracts only relevant information from documents

### User Experience
- ğŸ¨ **Modern UI** - Responsive, intuitive interface with dark mode support
- ğŸ“± **Mobile Friendly** - Works seamlessly on all devices
- ğŸ“„ **PDF Export** - Download analysis reports as PDF
- ğŸ• **Query History** - Track and reuse previous queries
- âš¡ **Real-time Processing** - See query status and cache indicators

### Production Ready
- ğŸš€ **FastAPI Backend** - High-performance async API
- ğŸ³ **Docker Support** - Easy deployment with containers
- â˜ï¸ **Cloud Ready** - Deployable to Hugging Face Spaces
- ğŸ“ˆ **Monitoring** - Cache statistics and performance metrics
- ğŸ”’ **Secure** - API key management and CORS configuration
- âš™ï¸ **Configurable** - Extensive configuration options

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         User Query                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Query Result Cache                            â”‚
â”‚                  (Check for cached answer)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Cache MISS                            â”‚ Cache HIT
         â–¼                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Query Expansion        â”‚          â”‚  Return Cached       â”‚
â”‚   (Generate variations)  â”‚          â”‚  Answer (instant)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Embedding Cache                                â”‚
â”‚              (Cache query embeddings)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Zilliz Vector Search                             â”‚
â”‚         (Retrieve top 30 relevant documents)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MMR Reranking                                  â”‚
â”‚          (Select best 10 with diversity)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Contextual Compression                              â”‚
â”‚          (Extract relevant sentences)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Conversation History Integration                        â”‚
â”‚         (Add context from previous exchanges)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  OpenAI GPT-4o Generation                         â”‚
â”‚            (Generate answer with citations)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Cache & Return                                 â”‚
â”‚         (Store in cache for future queries)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ›  Technology Stack

### Backend
- **Framework**: FastAPI 0.115.0 - Modern, high-performance Python web framework
- **AI/ML**: 
  - LangChain 0.3.7 - Advanced RAG orchestration
  - OpenAI GPT-4o - Latest language model for generation
  - OpenAI text-embedding-3-large - 3072-dimensional embeddings
- **Vector Database**: Zilliz Cloud (Milvus) - Scalable vector similarity search
- **Python**: 3.11+ - Latest stable version with performance improvements

### Frontend
- **HTML5/CSS3** - Modern, responsive design
- **Vanilla JavaScript** - No framework overhead, fast performance
- **jsPDF** - Client-side PDF generation

### Infrastructure
- **Docker** - Containerization for consistent deployments
- **Uvicorn** - ASGI server for FastAPI
- **Pydantic** - Data validation and settings management

## ğŸ’¡ What I Built

### 1. Core RAG Pipeline

#### Hybrid Search System
I implemented a sophisticated retrieval system that combines:
- **Semantic Search**: Uses 3072-dimensional embeddings for meaning-based retrieval
- **Metadata Filtering**: Filters by ticker symbol, document type, and date
- **Zilliz Integration**: Connects to cloud-hosted vector database for scalability

```python
# Example: Retrieve documents with filters
documents = retriever.retrieve(
    query="What was revenue in 2024?",
    ticker="ACM",
    doc_types=["income_statement"],
    top_k=30
)
```

#### Query Expansion
Automatically generates alternative phrasings to capture different ways users might ask questions:
- "What was revenue?" â†’ "What was total revenue?", "Show me sales figures"
- Uses GPT-4o to understand financial terminology
- Improves recall by 30-40%

#### MMR Reranking
Implemented Maximal Marginal Relevance algorithm to:
- Balance relevance vs. diversity in results
- Retrieve 30 documents, rerank to best 10
- Prevents redundant information

#### Contextual Compression
Built an LLM-powered compression system that:
- Extracts only relevant sentences from retrieved chunks
- Reduces context size by 40-60%
- Maintains critical financial figures and context

### 2. Conversation Memory System

#### Multi-Turn Conversations
Created a session-based memory system enabling:
- **Follow-up questions**: "What about 2025?" after asking about 2024
- **Pronoun resolution**: Understands "it", "that", "them" references
- **Context retention**: Maintains last 3 exchanges (6 messages)
- **Automatic trimming**: Prevents context overflow with smart token management

```python
# Example conversation flow
Q1: "What was ACM's revenue in 2024?"
A1: "$16.11B [Source 1]"

Q2: "What about 2025?"  # References previous query
A2: "$16.14B, up 0.19% from 2024 [Source 1]"

Q3: "Calculate the growth rate"  # Uses both previous answers
A3: "0.19% year-over-year growth..."
```

#### Session Management
- **Browser-based sessions**: Unique ID per browser tab
- **Session persistence**: Survives page refresh (sessionStorage)
- **Server-side tracking**: Backend maintains conversation state
- **Clean separation**: Each tab has independent conversation

#### Smart Context Management
- **Token limits**: Maximum 4000 tokens for conversation history
- **Automatic pruning**: Removes oldest messages when limit reached
- **Relevance preservation**: Keeps most recent exchanges
- **Performance optimization**: Truncates messages to 300 characters in prompts

### 3. Multi-Layer Caching System

I built a comprehensive caching system that dramatically reduces costs and improves speed:

#### Embedding Cache
```python
class EmbeddingCache:
    # Caches OpenAI embeddings to avoid redundant API calls
    # - 24-hour TTL
    # - 1000 entry capacity
    # - SHA-256 key hashing
    # - LRU eviction policy
```

**Benefits**:
- Saves 50-80% of embedding API costs
- Reduces latency by ~200ms per cached embedding
- Handles both query and document embeddings

#### Query Result Cache
```python
class QueryResultCache:
    # Caches complete RAG responses
    # - 1-hour TTL
    # - 100 entry capacity
    # - Smart invalidation for follow-ups
    # - Cost tracking
```

**Benefits**:
- Returns instant results (<50ms) for repeated queries
- Saves ~$0.01-0.02 per cached query
- Tracks estimated cost savings

#### Smart Cache Invalidation
Automatically detects follow-up questions and skips cache:
```python
follow_up_indicators = [
    "what about", "how about", "and", "that", "it",
    "compared to", "vs", "previous", "earlier"
]
```

**Example**:
- "What was revenue?" â†’ Uses cache âœ“
- "What about 2025?" â†’ Skips cache (needs context) âœ“
- "Calculate current ratio" â†’ Uses cache âœ“

### 4. Production Features

#### Comprehensive System Prompt
Engineered a detailed 400-line system prompt that:
- Specifies exact financial statement labels to use
- Provides formulas for 19 different financial ratios
- Includes example calculations with real numbers
- Defines output format and citation rules
- Prevents hallucination with strict guidelines

#### Citation Tracking
Built a citation system that:
- Assigns unique [Source N] identifiers
- Tracks document metadata (filename, type, similarity)
- Deduplicates sources
- Displays source previews
- Links citations to original documents

#### Error Handling
Implemented robust error handling:
- Connection timeouts (30s for LLM calls)
- Graceful degradation (caching failures)
- Detailed error messages
- Retry logic for transient failures
- Health check endpoints

#### API Endpoints
Created RESTful API with:
- `POST /query` - Main query endpoint
- `GET /health` - Health check
- `GET /stats` - Collection statistics
- `GET /cache/stats` - Cache performance metrics
- `DELETE /cache/clear` - Manual cache clearing
- `DELETE /session/{id}` - Session cleanup

## ğŸ“ Project Structure

```
finsight-rag/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py                 # Package initialization
â”‚   â”‚   â”œâ”€â”€ main.py                     # FastAPI application entry
â”‚   â”‚   â”œâ”€â”€ config.py                   # Configuration management
â”‚   â”‚   â”œâ”€â”€ models.py                   # Pydantic models
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ rag/                        # RAG pipeline components
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ chain.py                # Main RAG orchestration
â”‚   â”‚   â”‚   â”œâ”€â”€ retriever.py            # Zilliz vector search
â”‚   â”‚   â”‚   â”œâ”€â”€ query_expander.py       # Query variation generation
â”‚   â”‚   â”‚   â”œâ”€â”€ reranker.py             # MMR reranking algorithm
â”‚   â”‚   â”‚   â””â”€â”€ compressor.py           # Contextual compression
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ utils/                      # Utility functions
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ citations.py            # Source citation tracking
â”‚   â”‚       â”œâ”€â”€ conversation.py         # Conversation memory
â”‚   â”‚       â””â”€â”€ cache.py                # Multi-layer caching
â”‚   â”‚
â”‚   â”œâ”€â”€ requirements.txt                # Python dependencies
â”‚   â””â”€â”€ .env.example                    # Environment variables template
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html                      # Main UI
â”‚   â”œâ”€â”€ style.css                       # Styling
â”‚   â””â”€â”€ script.js                       # Frontend logic
â”‚
â”œâ”€â”€ docs/                               # Documentation
â”‚   â”œâ”€â”€ CONVERSATION_MEMORY_GUIDE.md    # Memory system guide
â”‚   â”œâ”€â”€ CACHING_GUIDE.md                # Caching system guide
â”‚   â”œâ”€â”€ HUGGINGFACE_DEPLOYMENT_GUIDE.md # Deployment instructions
â”‚   â””â”€â”€ QUICK_START.md                  # Getting started guide
â”‚
â”œâ”€â”€ Dockerfile                          # Docker configuration
â”œâ”€â”€ .dockerignore                       # Docker ignore rules
â”œâ”€â”€ .gitignore                          # Git ignore rules
â”œâ”€â”€ LICENSE                             # MIT License
â””â”€â”€ README.md                           # This file
```

## ğŸš€ Installation

### Prerequisites

- Python 3.11 or higher
- OpenAI API key ([Get one here](https://platform.openai.com/api-keys))
- Zilliz Cloud account ([Sign up](https://cloud.zilliz.com/))
- Git

### Local Setup

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/finsight-rag.git
cd finsight-rag
```

2. **Create virtual environment**
```bash
cd backend
python -m venv venv

# Activate virtual environment
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Configure environment variables**
```bash
# Copy example file
cp .env.example .env

# Edit .env and add your API keys:
# OPENAI_API_KEY=sk-proj-...
# ZILLIZ_URI=https://...
# ZILLIZ_TOKEN=db_admin:...
```

5. **Run the application**
```bash
uvicorn app.main:app --reload --port 8000
```

6. **Open in browser**
```
Frontend: http://localhost:8000
API Docs: http://localhost:8000/docs
```

### Docker Setup

```bash
# Build image
docker build -t finsight-rag .

# Run container
docker run -p 7860:7860 \
  -e OPENAI_API_KEY=your_key \
  -e ZILLIZ_URI=your_uri \
  -e ZILLIZ_TOKEN=your_token \
  finsight-rag
```

## âš™ï¸ Configuration

### Environment Variables

Create a `.env` file in the `backend/` directory:

```env
# Required - OpenAI
OPENAI_API_KEY=sk-proj-...
OPENAI_MODEL=gpt-4o
OPENAI_EMBEDDING_MODEL=text-embedding-3-large

# Required - Zilliz
ZILLIZ_URI=https://in03-xxxxx.api.gcp-us-west1.zillizcloud.com
ZILLIZ_TOKEN=db_admin:xxxxx
COLLECTION_NAME=financial_documents

# Optional - RAG Settings
DEFAULT_TOP_K=10                    # Number of sources to retrieve
RETRIEVAL_TOP_K=30                  # Documents before reranking
ENABLE_QUERY_EXPANSION=True         # Generate query variations
ENABLE_RERANKING=True               # Use MMR reranking
ENABLE_COMPRESSION=True             # Compress retrieved content

# Optional - Caching
ENABLE_QUERY_CACHE=True             # Cache complete responses
ENABLE_EMBEDDING_CACHE=True         # Cache embeddings
EMBEDDING_CACHE_SIZE=1000           # Max cached embeddings
EMBEDDING_CACHE_TTL=86400           # 24 hours
QUERY_CACHE_SIZE=100                # Max cached queries
QUERY_CACHE_TTL=3600                # 1 hour

# Optional - Server
PORT=8000                           # Server port
```

### Feature Toggles

Disable features for faster responses or lower costs:

```python
# Faster but less comprehensive
ENABLE_QUERY_EXPANSION=False        # Saves ~1-2s per query
ENABLE_COMPRESSION=False            # Saves ~0.5-1s per query
ENABLE_RERANKING=False              # Saves ~0.3-0.5s per query

# Cost optimization
EMBEDDING_CACHE_TTL=172800          # 48 hours (more caching)
QUERY_CACHE_TTL=7200                # 2 hours (more caching)
```

## ğŸ“– Usage

### Web Interface

1. Navigate to `http://localhost:8000`
2. Enter your financial question
3. Click "Ask Question" or press `Ctrl+Enter`
4. View detailed answer with sources
5. Ask follow-up questions in the same session

### Example Queries

**Basic Questions**:
```
- What was ACM's revenue in 2024?
- Show me the balance sheet for 2025
- What are ACM's total assets?
```

**Financial Ratios**:
```
- Calculate ACM's current ratio
- What is the debt-to-equity ratio?
- Show me the net profit margin
```

**Trend Analysis**:
```
- Analyze revenue trends over 3 years
- How has profitability changed?
- Compare 2024 vs 2025 performance
```

**Follow-up Questions**:
```
Q: "What was revenue in 2024?"
A: "$16.11B"

Q: "What about 2025?"        # Uses context
Q: "Calculate the growth rate"  # Uses both answers
Q: "Is that good or bad?"      # Analyzes the metric
```

### API Usage

```python
import requests

# Query endpoint
response = requests.post(
    "http://localhost:8000/query",
    json={
        "query": "What was ACM's revenue in 2024?",
        "ticker": "ACM",
        "doc_types": ["income_statement"],
        "top_k": 10,
        "session_id": "my_session_123"
    }
)

result = response.json()
print(f"Answer: {result['answer']}")
print(f"Sources: {len(result['sources'])}")
print(f"From cache: {result.get('from_cache', False)}")
```

## ğŸ“š API Documentation

### POST /query

Process a financial query and return answer with sources.

**Request**:
```json
{
  "query": "What was ACM's revenue in 2024?",
  "ticker": "ACM",
  "doc_types": ["income_statement", "balance_sheet"],
  "top_k": 10,
  "session_id": "optional_session_id"
}
```

**Response**:
```json
{
  "answer": "ACM's revenue in FY 2024 was $16.11B [Source 1]...",
  "sources": [
    {
      "source_id": 1,
      "filename": "ACM_income_statement.md",
      "doc_type": "income_statement",
      "ticker": "ACM",
      "similarity_score": 0.92,
      "chunk_id": "chunk_0",
      "text_preview": "Contract Revenue FY 2024: $16.11B..."
    }
  ],
  "query": "What was ACM's revenue in 2024?",
  "processing_time": 2.34,
  "expanded_queries": ["What was ACM's revenue in 2024?"],
  "num_documents_retrieved": 5,
  "session_id": "session_123",
  "from_cache": false
}
```

### GET /health

Health check endpoint.

**Response**:
```json
{
  "status": "healthy",
  "version": "1.0.0"
}
```

### GET /cache/stats

Get cache performance metrics.

**Response**:
```json
{
  "embedding_cache": {
    "hit_rate": 63.72,
    "size": 234,
    "hits": 1567,
    "misses": 892,
    "estimated_cost_savings_usd": 0.15
  },
  "query_result_cache": {
    "hit_rate": 36.33,
    "size": 45,
    "hits": 89,
    "misses": 156,
    "estimated_cost_savings_usd": 0.89
  }
}
```

### Interactive Documentation

Visit `http://localhost:8000/docs` for full Swagger UI documentation with:
- Interactive API testing
- Request/response schemas
- Authentication details
- Example requests

## ğŸš¢ Deployment

### Hugging Face Spaces (Recommended)

1. **Create Space**
   - Go to [Hugging Face Spaces](https://huggingface.co/spaces)
   - Click "Create new Space"
   - Choose Docker SDK

2. **Add Secrets**
   - Settings â†’ Repository secrets
   - Add: `OPENAI_API_KEY`, `ZILLIZ_URI`, `ZILLIZ_TOKEN`

3. **Push Code**
   ```bash
   git clone https://huggingface.co/spaces/YOUR_USERNAME/finsight-rag
   cd finsight-rag
   # Copy files
   git add .
   git commit -m "Deploy"
   git push
   ```

4. **Wait for Build** (5-10 minutes)

5. **Your app is live!**
   ```
   https://huggingface.co/spaces/YOUR_USERNAME/finsight-rag
   ```

See [HUGGINGFACE_DEPLOYMENT_GUIDE.md](docs/HUGGINGFACE_DEPLOYMENT_GUIDE.md) for detailed instructions.

### Other Platforms

- **AWS**: Use ECS/Fargate with Docker image
- **Google Cloud**: Deploy to Cloud Run
- **Azure**: Use Azure Container Instances
- **Heroku**: Use container registry

## ğŸ“Š Performance & Cost Analysis

### Response Times

| Query Type | Without Cache | With Cache | Improvement |
|------------|---------------|------------|-------------|
| Simple | 2-4s | <100ms | **95% faster** |
| Complex | 5-10s | <100ms | **98% faster** |
| With Expansion | 8-15s | <100ms | **99% faster** |

### Cost Analysis

**Without Caching**:
- Embeddings (3 calls): $0.0003
- LLM generation: $0.015
- Query expansion: $0.002
- Compression: $0.003
- **Total**: ~$0.020 per query

**With Caching (50% hit rate)**:
- 50% cached: $0
- 50% new: $0.01
- **Average**: $0.01 per query
- **Savings**: 50% ($30/month for 100 queries/day)

**With Caching (70% hit rate)**:
- **Average**: $0.006 per query
- **Savings**: 70% ($42/month for 100 queries/day)

### Monthly Cost Estimates

| Queries/Day | Without Cache | 50% Cache Hit | 70% Cache Hit | Savings |
|-------------|---------------|---------------|---------------|---------|
| 50 | $30 | $15 | $9 | $21/mo |
| 100 | $60 | $30 | $18 | $42/mo |
| 500 | $300 | $150 | $90 | $210/mo |
| 1000 | $600 | $300 | $180 | $420/mo |

### Performance Optimizations

1. **Caching** - Reduces 40-70% of API calls
2. **Async Operations** - Parallel processing where possible
3. **Connection Pooling** - Reuses database connections
4. **Compression** - Reduces context size by 40-60%
5. **Smart Reranking** - Processes only top candidates

## ğŸ¯ Advanced Features

### Conversation Context

The system maintains conversational context:

```
User: "What was revenue in 2024?"
AI: "$16.11B from contract revenue [Source 1]"

User: "What about 2025?"
AI: "Revenue in FY 2025 was $16.14B, an increase of $30M (0.19%) 
     compared to the $16.11B reported in FY 2024 [Source 1]"

User: "Is that good?"
AI: "A 0.19% increase indicates relatively flat growth..."
```

### Financial Ratio Calculations

Automatically calculates and explains 19 financial ratios:

- **Liquidity**: Current Ratio, Quick Ratio, Working Capital
- **Leverage**: Debt-to-Equity, Debt-to-Assets, Equity Ratio
- **Profitability**: Gross/Operating/Net Margins, ROA, ROE
- **Efficiency**: Asset Turnover, Inventory Turnover
- **Cash Flow**: OCF Margin, Free Cash Flow, CF to Income Ratio

### Query Expansion Examples

Input: `"What was revenue in 2024?"`

Expanded to:
- "What was total revenue in fiscal year 2024?"
- "Show me contract revenue for 2024"
- "What were the sales figures for FY 2024?"

### Citation System

Every answer includes source references:

```
Direct Answer: ACM's current ratio was 1.13 in FY 2025 [Source 1].

Calculation:
â€¢ Formula: Current Assets Ã· Current Liabilities
â€¢ FY 2025: $6.73B Ã· $5.93B = 1.13 [Source 1]

Sources:
[Source 1] ACM_balance_sheet.md (Balance Sheet, Similarity: 94%)
```

## ğŸ”§ Troubleshooting

### Common Issues

**1. "Failed to initialize RAG chain"**
- Check API keys are set correctly
- Verify Zilliz connection (URI and token)
- Ensure collection `financial_documents` exists

**2. Slow responses**
- Enable caching if disabled
- Check internet connection
- Consider disabling query expansion for speed

**3. "Out of memory" errors**
- Reduce cache sizes in config
- Decrease `RETRIEVAL_TOP_K`
- Disable compression temporarily

**4. Cache not working**
- Verify `ENABLE_QUERY_CACHE=True`
- Check `/cache/stats` endpoint
- Clear cache with `DELETE /cache/clear`

**5. Frontend can't connect to API**
- Ensure backend is running
- Check CORS settings in `config.py`
- Verify port in `script.js` matches server

### Debug Mode

Enable detailed logging:

```python
# In config.py
import logging
logging.basicConfig(level=logging.DEBUG)
```

### Health Checks

```bash
# Check API
curl http://localhost:8000/health

# Check cache stats
curl http://localhost:8000/cache/stats

# Check collection stats
curl http://localhost:8000/stats
```

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development Guidelines

- Follow PEP 8 style guide
- Add docstrings to all functions
- Write tests for new features
- Update documentation
- Ensure all tests pass

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **LangChain** - Excellent RAG framework
- **OpenAI** - Powerful language models
- **Zilliz** - Scalable vector database
- **FastAPI** - Modern Python web framework
- **Hugging Face** - Easy deployment platform

## ğŸ“ Contact

- **Author**: Adeyemi
- **GitHub**: [@Adeyemi0](https://github.com/Adeyemi0)
- **Project Link**: [https://github.com/Adeyemi0/FinSight-RAG-Application-](https://github.com/Adeyemi0/FinSight-RAG-Application-)

## ğŸŒŸ Star History

If you find this project helpful, please consider giving it a star â­
